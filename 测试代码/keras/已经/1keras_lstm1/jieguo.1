测试1

model.add(LSTM(1, input_shape=(50, 6)))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
model.fit(X_train, y_train, batch_size=32, nb_epoch=100, validation_split=0.2, show_accuracy=True)
score = model.evaluate(X_test, y_test, batch_size=32)



loss: 0.6532 - val_loss: 
/home/tangdongge/anaconda2/bin/python /home/tangdongge/Stock-Market/测试代码/keras/stock_lstm.py
Using TensorFlow backend.
/home/tangdongge/anaconda2/lib/python2.7/site-packages/keras/models.py:610: UserWarning: The "show_accuracy" argument is deprecated, instead you should pass the "accuracy" metric to the model at compile time:
Train...
`model.compile(optimizer, loss, metrics=["accuracy"])`
  warnings.warn('The "show_accuracy" argument is deprecated, '
Train on 7421 samples, validate on 1856 samples
Epoch 1/100
7421/7421 [==============================] - 14s - loss: 0.7110 - val_loss: 0.6641
Epoch 2/100
7421/7421 [==============================] - 14s - loss: 0.6801 - val_loss: 0.6632
Epoch 3/100
7421/7421 [==============================] - 14s - loss: 0.6699 - val_loss: 0.6627
Epoch 4/100
7421/7421 [==============================] - 14s - loss: 0.6600 - val_loss: 0.6631
Epoch 5/100
7421/7421 [==============================] - 14s - loss: 0.6586 - val_loss: 0.6624
Epoch 6/100
7421/7421 [==============================] - 14s - loss: 0.6566 - val_loss: 0.6622
Epoch 7/100
7421/7421 [==============================] - 14s - loss: 0.6551 - val_loss: 0.6631
Epoch 8/100
7421/7421 [==============================] - 14s - loss: 0.6544 - val_loss: 0.6619
Epoch 9/100
7421/7421 [==============================] - 14s - loss: 0.6548 - val_loss: 0.6622
Epoch 10/100
7421/7421 [==============================] - 14s - loss: 0.6532 - val_loss: 0.6620
Epoch 11/100
7421/7421 [==============================] - 14s - loss: 0.6537 - val_loss: 0.6618
Epoch 12/100
7421/7421 [==============================] - 14s - loss: 0.6532 - val_loss: 0.6630
Epoch 13/100
7421/7421 [==============================] - 15s - loss: 0.6546 - val_loss: 0.6620
Epoch 14/100
7421/7421 [==============================] - 16s - loss: 0.6544 - val_loss: 0.6622
Epoch 15/100
7421/7421 [==============================] - 16s - loss: 0.6533 - val_loss: 0.6618
Epoch 16/100
7421/7421 [==============================] - 17s - loss: 0.6537 - val_loss: 0.6616
Epoch 17/100
7421/7421 [==============================] - 16s - loss: 0.6538 - val_loss: 0.6620
Epoch 18/100
7421/7421 [==============================] - 16s - loss: 0.6534 - val_loss: 0.6627
Epoch 19/100
7421/7421 [==============================] - 16s - loss: 0.6531 - val_loss: 0.6621
Epoch 20/100
7421/7421 [==============================] - 17s - loss: 0.6531 - val_loss: 0.6617
Epoch 21/100
7421/7421 [==============================] - 17s - loss: 0.6529 - val_loss: 0.6619
Epoch 22/100
7421/7421 [==============================] - 18s - loss: 0.6537 - val_loss: 0.6619
Epoch 23/100
7421/7421 [==============================] - 16s - loss: 0.6521 - val_loss: 0.6616
Epoch 24/100
7421/7421 [==============================] - 17s - loss: 0.6534 - val_loss: 0.6618
Epoch 25/100
7421/7421 [==============================] - 17s - loss: 0.6530 - val_loss: 0.6621
Epoch 26/100
7421/7421 [==============================] - 17s - loss: 0.6528 - val_loss: 0.6618
Epoch 27/100
7421/7421 [==============================] - 17s - loss: 0.6533 - val_loss: 0.6622
Epoch 28/100
7421/7421 [==============================] - 16s - loss: 0.6532 - val_loss: 0.6617
Epoch 29/100
7421/7421 [==============================] - 17s - loss: 0.6522 - val_loss: 0.6619
Epoch 30/100
7421/7421 [==============================] - 17s - loss: 0.6533 - val_loss: 0.6621
Epoch 31/100
7421/7421 [==============================] - 16s - loss: 0.6528 - val_loss: 0.6623
Epoch 32/100
7421/7421 [==============================] - 17s - loss: 0.6527 - val_loss: 0.6618
Epoch 33/100
7421/7421 [==============================] - 21s - loss: 0.6527 - val_loss: 0.6617
Epoch 34/100
7421/7421 [==============================] - 19s - loss: 0.6536 - val_loss: 0.6617
Epoch 35/100
7421/7421 [==============================] - 18s - loss: 0.6526 - val_loss: 0.6614
Epoch 36/100
7421/7421 [==============================] - 18s - loss: 0.6533 - val_loss: 0.6623
Epoch 37/100
7421/7421 [==============================] - 17s - loss: 0.6528 - val_loss: 0.6617
Epoch 38/100
7421/7421 [==============================] - 17s - loss: 0.6527 - val_loss: 0.6629
Epoch 39/100
7421/7421 [==============================] - 17s - loss: 0.6530 - val_loss: 0.6616
Epoch 40/100
7421/7421 [==============================] - 17s - loss: 0.6531 - val_loss: 0.6618
Epoch 41/100
7421/7421 [==============================] - 17s - loss: 0.6524 - val_loss: 0.6616
Epoch 42/100
7421/7421 [==============================] - 17s - loss: 0.6530 - val_loss: 0.6626
Epoch 43/100
7421/7421 [==============================] - 17s - loss: 0.6521 - val_loss: 0.6614
Epoch 44/100
7421/7421 [==============================] - 17s - loss: 0.6536 - val_loss: 0.6618
Epoch 45/100
7421/7421 [==============================] - 17s - loss: 0.6532 - val_loss: 0.6618
Epoch 46/100
7421/7421 [==============================] - 17s - loss: 0.6534 - val_loss: 0.6622
Epoch 47/100
7421/7421 [==============================] - 18s - loss: 0.6528 - val_loss: 0.6618
Epoch 48/100
7421/7421 [==============================] - 18s - loss: 0.6527 - val_loss: 0.6615
Epoch 49/100
7421/7421 [==============================] - 21s - loss: 0.6529 - val_loss: 0.6620
Epoch 50/100
7421/7421 [==============================] - 19s - loss: 0.6525 - val_loss: 0.6618
Epoch 51/100
7421/7421 [==============================] - 19s - loss: 0.6530 - val_loss: 0.6620
Epoch 52/100
7421/7421 [==============================] - 18s - loss: 0.6526 - val_loss: 0.6619
Epoch 53/100
7421/7421 [==============================] - 18s - loss: 0.6523 - val_loss: 0.6617
Epoch 54/100
7421/7421 [==============================] - 18s - loss: 0.6516 - val_loss: 0.6618
Epoch 55/100
7421/7421 [==============================] - 17s - loss: 0.6532 - val_loss: 0.6616
Epoch 56/100
7421/7421 [==============================] - 20s - loss: 0.6528 - val_loss: 0.6621
Epoch 57/100
7421/7421 [==============================] - 30s - loss: 0.6532 - val_loss: 0.6620
Epoch 58/100
7421/7421 [==============================] - 31s - loss: 0.6525 - val_loss: 0.6621
Epoch 59/100
7421/7421 [==============================] - 31s - loss: 0.6522 - val_loss: 0.6618
Epoch 60/100
7421/7421 [==============================] - 27s - loss: 0.6523 - val_loss: 0.6618
Epoch 61/100
7421/7421 [==============================] - 27s - loss: 0.6523 - val_loss: 0.6623
Epoch 62/100
7421/7421 [==============================] - 28s - loss: 0.6532 - val_loss: 0.6615
Epoch 63/100
7421/7421 [==============================] - 26s - loss: 0.6526 - val_loss: 0.6620
Epoch 64/100
7421/7421 [==============================] - 27s - loss: 0.6522 - val_loss: 0.6620
Epoch 65/100
7421/7421 [==============================] - 24s - loss: 0.6523 - val_loss: 0.6617
Epoch 66/100
7421/7421 [==============================] - 25s - loss: 0.6531 - val_loss: 0.6616
Epoch 67/100
7421/7421 [==============================] - 25s - loss: 0.6526 - val_loss: 0.6621
Epoch 68/100
7421/7421 [==============================] - 26s - loss: 0.6525 - val_loss: 0.6624
Epoch 69/100
7421/7421 [==============================] - 26s - loss: 0.6524 - val_loss: 0.6614
Epoch 70/100
7421/7421 [==============================] - 26s - loss: 0.6523 - val_loss: 0.6621
Epoch 71/100
7421/7421 [==============================] - 26s - loss: 0.6522 - val_loss: 0.6621
Epoch 72/100
7421/7421 [==============================] - 25s - loss: 0.6521 - val_loss: 0.6614
Epoch 73/100
7421/7421 [==============================] - 25s - loss: 0.6527 - val_loss: 0.6619
Epoch 74/100
7421/7421 [==============================] - 25s - loss: 0.6527 - val_loss: 0.6617
Epoch 75/100
7421/7421 [==============================] - 26s - loss: 0.6521 - val_loss: 0.6616
Epoch 76/100
7421/7421 [==============================] - 27s - loss: 0.6527 - val_loss: 0.6615
Epoch 77/100
7421/7421 [==============================] - 29s - loss: 0.6529 - val_loss: 0.6618
Epoch 78/100
7421/7421 [==============================] - 29s - loss: 0.6525 - val_loss: 0.6622
Epoch 79/100
7421/7421 [==============================] - 31s - loss: 0.6530 - val_loss: 0.6620
Epoch 80/100
7421/7421 [==============================] - 35s - loss: 0.6528 - val_loss: 0.6625
Epoch 81/100
7421/7421 [==============================] - 39s - loss: 0.6526 - val_loss: 0.6616
Epoch 82/100
7421/7421 [==============================] - 39s - loss: 0.6526 - val_loss: 0.6616
Epoch 83/100
7421/7421 [==============================] - 34s - loss: 0.6528 - val_loss: 0.6618
Epoch 84/100
7421/7421 [==============================] - 35s - loss: 0.6524 - val_loss: 0.6618
Epoch 85/100
7421/7421 [==============================] - 35s - loss: 0.6525 - val_loss: 0.6626
Epoch 86/100
7421/7421 [==============================] - 32s - loss: 0.6531 - val_loss: 0.6613
Epoch 87/100
7421/7421 [==============================] - 33s - loss: 0.6524 - val_loss: 0.6623
Epoch 88/100
7421/7421 [==============================] - 34s - loss: 0.6525 - val_loss: 0.6615
Epoch 89/100
7421/7421 [==============================] - 34s - loss: 0.6522 - val_loss: 0.6618
Epoch 90/100
7421/7421 [==============================] - 33s - loss: 0.6525 - val_loss: 0.6616
Epoch 91/100
7421/7421 [==============================] - 33s - loss: 0.6531 - val_loss: 0.6619
Epoch 92/100
7421/7421 [==============================] - 35s - loss: 0.6523 - val_loss: 0.6619
Epoch 93/100
7421/7421 [==============================] - 39s - loss: 0.6520 - val_loss: 0.6618
Epoch 94/100
7421/7421 [==============================] - 39s - loss: 0.6525 - val_loss: 0.6621
Epoch 95/100
7421/7421 [==============================] - 39s - loss: 0.6523 - val_loss: 0.6618
Epoch 96/100
7421/7421 [==============================] - 39s - loss: 0.6527 - val_loss: 0.6618
Epoch 97/100
7421/7421 [==============================] - 38s - loss: 0.6515 - val_loss: 0.6618
Epoch 98/100
7421/7421 [==============================] - 38s - loss: 0.6523 - val_loss: 0.6618
Epoch 99/100
7421/7421 [==============================] - 39s - loss: 0.6525 - val_loss: 0.6616
Epoch 100/100
7421/7421 [==============================] - 39s - loss: 0.6529 - val_loss: 0.6620
2000/2000 [==============================] - 2s     
Test score: 0.669365324974

Process finished with exit code 0























































测试2
model.add(LSTM(1, input_shape=(50, 6),consume_less='cpu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
print("Train...")
model.fit(X_train, y_train, batch_size=1, nb_epoch=50, validation_split=0.2, show_accuracy=True)
score = model.evaluate(X_test, y_test, batch_size=1)
print('Test score:', score)

  0.669742972374



































测试3
model.add(LSTM(1, input_shape=(50, 6)))
model.add(Dropout(0.2))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
print("Train...")
model.fit(X_train, y_train, batch_size=1, nb_epoch=50, validation_split=0.2, show_accuracy=True)
score = model.evaluate(X_test, y_test, batch_size=1)
print('Test score:', score)


0.66432020545


































测试4
model.add(LSTM(16, input_shape=(50, 6)))
model.add(Dropout(0.2))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
print("Train...")
model.fit(X_train, y_train, batch_size=16, nb_epoch=50, validation_split=0.2, show_accuracy=True)
score = model.evaluate(X_test, y_test, batch_size=16)
print('Test score:', score)


 0.658813778162


















测试5
model = Sequential()
model.add(LSTM(32, input_shape=(50, 6)))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
print("Train...")
model.fit(X_train, y_train, batch_size=1, nb_epoch=50, validation_split=0.2, show_accuracy=True)
score = model.evaluate(X_test, y_test, batch_size=1)
print('Test score:', score)


 0.65825185409

































测试6
model.add(Dropout(0.2))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
print("Train...")
model.fit(X_train, y_train, batch_size=8, nb_epoch=50, validation_split=0.2, show_accuracy=True)
score = model.evaluate(X_test, y_test, batch_size=8)
print('Test score:', score)

 0.661286864519
